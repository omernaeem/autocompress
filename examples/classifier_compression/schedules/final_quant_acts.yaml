version: 1
quantizers:
  linear_quantizer:
    class: 'QuantAwareTrainRangeLinearQuantizer'
    bits_activations : 32
    bits_weights : 32
    mode: 'ASYMMETRIC_UNSIGNED'
    ema_decay: 0.999
    per_channel_wts: True
    bits_overrides:
        conv1:
             wts: 8
        relu:
             acts: 8
        layer1.0.conv1:
             wts: 5
        layer1.0.relu1:
             acts: 5 
        layer1.0.conv2:
             wts: 4
        layer1.0.relu2:
             acts: 4  
        layer1.1.conv1:
             wts: 8 
        layer1.1.relu1:
             acts: 8  
        layer1.1.conv2:
             wts: 3
        layer1.1.relu2:
             acts: 3   
        layer1.2.conv1:
             wts: 5
        layer1.2.relu1:
             acts: 5   
        layer1.2.conv2:
             wts: 4
        layer1.2.relu2:
             acts: 4   
        layer2.0.conv1:
             wts: 4
        layer2.0.relu1:
             acts: 4   
        layer2.0.conv2:
             wts: 7 
        layer2.0.relu2:
             acts: 7  
        layer2.0.downsample.0:
             wts: 7 
        layer2.1.conv1:
             wts: 6
        layer2.1.relu1:
             acts: 6   
        layer2.1.conv2:
             wts: 3 
        layer2.1.relu2:
             acts: 3  
        layer2.2.conv1:
             wts: 3 
        layer2.2.relu1:
             acts: 2  
        layer2.2.conv2:
             wts: 3 
        layer2.2.relu2:
             acts: 3  
        layer3.0.conv1:
             wts: 3
        layer3.0.relu1:
             acts: 3   
        layer3.0.conv2:
             wts: 3 
        layer3.0.relu2:
             acts: 3  
        layer3.0.downsample.0:
             wts: 5 
        layer3.1.conv1:
             wts: 3 
        layer3.1.relu1:
             acts: 3  
        layer3.1.conv2:
             wts: 4 
        layer3.1.relu2:
             acts: 4  
        layer3.2.conv1:
             wts: 4
        layer3.2.relu1:
             acts: 4   
        layer3.2.conv2:
             wts: 2 
        layer3.2.relu2:
             acts: 2  
        fc:
             wts: 4 

lr_schedulers:
  pruning_lr:
    class: ExponentialLR
    gamma: 0.97

policies:

  - lr_scheduler:
      instance_name: pruning_lr
    starting_epoch: 1
    ending_epoch: 500
    frequency: 1


  - quantizer:
      instance_name : 'linear_quantizer'
    starting_epoch: 0
    ending_epoch: 500
    frequency: 1
